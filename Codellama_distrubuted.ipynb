{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, I provide a detailed guideline, **how one can donwload model weights for Meta's Codellama model and run `CodeLlama` from scratch without any external libraries alike Huggingface."
      ],
      "metadata": {
        "id": "KBZl20OrJddw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Caution:** this notebook is not intended for anyone with `RAM < 16/Up`. Also you can't run it on Google Colab free Edition"
      ],
      "metadata": {
        "id": "YJZLqsDmJ0vs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone the official `codellama` repository"
      ],
      "metadata": {
        "id": "OYlgJp56KIEM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pI12BpQoCAPU"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/facebookresearch/codellama.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "enter into the repo"
      ],
      "metadata": {
        "id": "RlhFa-j1KUYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd codellama"
      ],
      "metadata": {
        "id": "3s_zrgQ2KVui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "list all the directories and files inside the directory"
      ],
      "metadata": {
        "id": "7jlarf4_KYh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "id": "R3zYTQMmKcBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading the dataset. (When you run the below script, in the terminal, you'll be asked to input a link provided to you via email by Meta for accessing `Codellama` input that)"
      ],
      "metadata": {
        "id": "VR0KaC0IKfeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!bash download.sh"
      ],
      "metadata": {
        "id": "eXiAAPJoKskH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "note: in the model selection to be downloaded, end the model name with `,` like `7b,` (while prompted with model selection)"
      ],
      "metadata": {
        "id": "6Ub6pwWUKtQk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "now let's install all the dependences using pip...."
      ],
      "metadata": {
        "id": "OHTD_aqyK7Ws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ."
      ],
      "metadata": {
        "id": "8nSUY3CcK4Bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we come to the main part"
      ],
      "metadata": {
        "id": "BcKTLxkeLOXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "below, I have written a modified script based on `example.py` file present in `codellama` repo. It streamlines the process and as well as provide you with the opportunity to curate the code based on your hardware and requirements."
      ],
      "metadata": {
        "id": "HD8DqriVLRJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can obviously try choose to execute example.py file from command line but if you are having trouble, then you can't fix it. And let's be honest, if you know programming, its good to see some code : )"
      ],
      "metadata": {
        "id": "O-Q4BmORLi-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "\n",
        "# this is important due to pytorch distributed being used. Otherwise you'll encounter an error\n",
        "os.environ['MASTER_ADDR'] = 'localhost'\n",
        "os.environ['MASTER_PORT'] = '12355'\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
        "\n",
        "# Initialize the distributed environment\n",
        "torch.cuda.set_device(0)  # Set the desired GPU device\n",
        "dist.init_process_group(backend='nccl', init_method='env://', rank=0, world_size=1)\n",
        "\n",
        "\n",
        "from llama import Llama\n",
        "\n",
        "def generate_text_from_prompts(\n",
        "    ckpt_dir: str,\n",
        "    tokenizer_path: str,\n",
        "    prompts: list,\n",
        "    temperature: float = 0.2,\n",
        "    top_p: float = 0.9,\n",
        "    max_seq_len: int = 256,\n",
        "    max_batch_size: int = 4,\n",
        "    max_gen_len: Optional[int] = None,\n",
        "):\n",
        "    generator = Llama.build(\n",
        "        ckpt_dir=ckpt_dir,\n",
        "        tokenizer_path=tokenizer_path,\n",
        "        max_seq_len=max_seq_len,\n",
        "        max_batch_size=max_batch_size,\n",
        "    )\n",
        "\n",
        "    results = generator.text_completion(\n",
        "        prompts,\n",
        "        max_gen_len=max_gen_len,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "    )\n",
        "\n",
        "    generated_texts = []\n",
        "    for prompt, result in zip(prompts, results):\n",
        "        generated_texts.append(result['generation'])\n",
        "\n",
        "    return generated_texts\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    ckpt_dir = \"<directory where codellama model was saved>\"\n",
        "    tokenizer_path = \"<path to the tokenizer.model file>\"\n",
        "    prompts = [\n",
        "        \"import socket\\n\\ndef ping_exponential_backoff(host: str):\",\n",
        "        \"import argparse\\n\\n\",\n",
        "    ]\n",
        "\n",
        "    generated_texts = generate_text_from_prompts(\n",
        "        ckpt_dir, tokenizer_path, prompts\n",
        "    )\n",
        "\n",
        "    for prompt, generated_text in zip(prompts, generated_texts):\n",
        "        print(prompt)\n",
        "        print(f\"> {generated_text}\")\n",
        "        print(\"\\n==================================\\n\")\n"
      ],
      "metadata": {
        "id": "4xtWuCAZLynZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}